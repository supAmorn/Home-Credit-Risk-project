{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Implement ML and test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "data_path = '../data/raw/home-credit-default-risk/'\n",
    "processed_path = '../data/processed/'\n",
    "app_test = pd.read_csv(data_path + 'application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test dataset to match the training dataset for prediction\n",
    "app_test['YEARS_BIRTH'] = app_test['DAYS_BIRTH'] / -365\n",
    "app_test['YEARS_EMPLOYED'] = abs(app_test['DAYS_EMPLOYED'] / -365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exported _agg DataFrames\n",
    "bureau_agg = pd.read_csv(os.path.join(processed_path, 'bureau_agg.csv'))\n",
    "pre_appl_agg = pd.read_csv(os.path.join(processed_path, 'pre_appl_agg.csv'))\n",
    "pos_cash_agg = pd.read_csv(os.path.join(processed_path, 'pos_cash_agg.csv'))\n",
    "install_pay_agg = pd.read_csv(os.path.join(processed_path, 'install_pay_agg.csv'))\n",
    "credit_card_agg = pd.read_csv(os.path.join(processed_path, 'credit_card_agg.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, encoders, selected features, and imputer have been loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the final model from the file\n",
    "model_path  = '../models/' \n",
    "with open(model_path+'final_model.pkl', 'rb') as model_file:\n",
    "    final_model = pickle.load(model_file)\n",
    "\n",
    "# Load the one-hot encoders from the file\n",
    "with open(model_path+'one_hot_code_encoders.pkl', 'rb') as encoder_file:\n",
    "    one_hot_encoders = pickle.load(encoder_file)\n",
    "\n",
    "# Load the label encoders from the file\n",
    "with open(model_path+'label_encoders.pkl', 'rb') as encoder_file:\n",
    "    label_encoders = pickle.load(encoder_file)\n",
    "\n",
    "# Load the imputer from the file\n",
    "with open(model_path+'imputer.pkl', 'rb') as imputer_file:\n",
    "    imputer = pickle.load(imputer_file)\n",
    "\n",
    "# Load the selected Features from the file\n",
    "with open(model_path+'selected_features.pkl', 'rb') as selected_features_file:\n",
    "    selected_features = pickle.load(selected_features_file)\n",
    "\n",
    "print(\"Model, encoders, selected features, and imputer have been loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames to merge\n",
    "dfs = [bureau_agg, pre_appl_agg, pos_cash_agg, install_pay_agg, credit_card_agg]\n",
    "\n",
    "# Merge all DataFrames in the list on 'SK_ID_CURR' using reduce and lambda\n",
    "final_merge_data = reduce(lambda left, right: left.merge(right, on='SK_ID_CURR', how='left'), [app_test] + dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48744, 123), (48744, 458))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the final merged data\n",
    "app_test.shape, final_merge_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the label encoders to the test dataset\n",
    "def apply_label_encoders(df, label_encoders, one_hot_encoders):\n",
    "    \"\"\"\n",
    "    Applies the label encoders to the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    label_encoders (dict): A dictionary of label encoders.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with encoded categorical features.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    # Apply Label Encoding   \n",
    "    for column, encoder in label_encoders.items():\n",
    "        df_encoded[column] = encoder.transform(df_encoded[column])\n",
    "        \n",
    "    # Apply One-Hot Encoding\n",
    "    for col in one_hot_encoders:\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=[col], drop_first=True)\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the test dataset\n",
    "test_encoded_data = apply_label_encoders(final_merge_data, label_encoders,one_hot_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns by using selected feature from training dataset\n",
    "test_data_selected = test_encoded_data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply imputation on selected features from the training dataset\n",
    "test_data_imputed = pd.DataFrame(imputer.transform(test_data_selected), columns=selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the loaded model\n",
    "y_test_pred = final_model.predict(test_data_imputed)\n",
    "y_test_proba = final_model.predict_proba(test_data_imputed)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\"SK_ID_CURR\": app_test[\"SK_ID_CURR\"], \"TARGET\": y_test_proba})\n",
    "submission.to_csv(\"../data/processed/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Upload the result into Kaggle to test the score**\n",
    "\n",
    "![Submission result from Kaggle](../reports/figures/Kaggle_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Propose methodologies for evaluating business impact and tracking model performance post-deployment.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating Business Impact**\n",
    "\n",
    "#### **1. Assumption**  \n",
    "- Baseline AUC: **0.5** (random guessing)  \n",
    "- Best Model AUC: **~0.77** (significantly better at detecting defaulters)  \n",
    "\n",
    "#### **2. Key Business Metrics**  \n",
    "- **Default Rate Detected:**  \n",
    "  - Measure how effectively the model identifies high-risk applicants.  \n",
    "- **Reduction in Bad Debt:**  \n",
    "  - Compare the decrease in non-performing loans (NPL) due to improved risk assessment.  \n",
    "\n",
    "#### **3. Approach: A/B Testing**  \n",
    "- **Group A (Control):** Uses the existing credit risk assessment process.  \n",
    "- **Group B (Test):** Uses the ML model for loan approvals.  \n",
    "- **Evaluation Period:** Monitor financial impact over 3-6 months.  \n",
    "- **Expected Outcome:** If Group B shows a significant reduction in bad debt while maintaining revenue growth, the ML model is validated for full deployment.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tracking Model Performance Post-Deployment**  \n",
    "\n",
    "#### **1. ROC-AUC Monitoring**  \n",
    "- Continuously track **ROC-AUC** to ensure the model maintains predictive power over time.  \n",
    "\n",
    "#### **2. Drift Detection**  \n",
    "- **Feature Drift:** Monitor changes in key features affecting loan approvals or feature distribution changed. \n",
    "- **Concept Drift:** Detect shifts in relationships among variables due to economic changes.  \n",
    "- **Feedback Loops:** Analyze false positives (incorrectly flagged as risky) and false negatives (missed defaulters) to refine the model.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Deployment Improvement Strategies**  \n",
    "\n",
    "#### **1. Periodic Model Retraining**  \n",
    "- Retrain the model using the latest data to account for changes in customer behavior and market trends.\n",
    "For example, when the distribution of data has been changed or time-period like quartly.  \n",
    "- Adjust hyperparameters and feature selection based on new insights.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**  \n",
    "By combining **A/B testing** for business validation, **drift detection**, and **ongoing monitoring**, we ensure that the ML model remains effective and drives measurable business impact. This approach helps align technical model performance with **key financial and operational objectives**, ensuring long-term success.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (home_credit_risk)",
   "language": "python",
   "name": "home_credit_risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
